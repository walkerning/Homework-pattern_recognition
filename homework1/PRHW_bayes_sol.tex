\documentclass{article}
\usepackage{scrextend}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{color}
\usepackage{bm}
%\include{macros}
%\usepackage{floatflt}
%\usepackage{graphics}
%\usepackage{epsfig}


\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem*{defition}{Definition}
\newtheorem*{example}{Example}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{note}{Note}
\newtheorem*{exercise}{Exercise}

\setlength{\oddsidemargin}{-0.25 in}
\setlength{\evensidemargin}{-0.25 in} \setlength{\topmargin}{-0.25
in} \setlength{\textwidth}{7 in} \setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.25 in} \setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

\newcommand{\homework}[4]{
\pagestyle{myheadings} \thispagestyle{plain}
\newpage
\setcounter{page}{1} \setcounter{section}{#4} \noindent
\begin{center}
\framebox{ \vbox{\vspace{2mm} \hbox to 6.28in { {\bf
THU-70250043,~Pattern~Recognition~(Spring 2017) \hfill Homework: 1} }
\vspace{6mm} \hbox to 6.28in { {\Large \hfill #1 \hfill} }
\vspace{6mm} \hbox to 6.28in { {\it Lecturer: #2 \hfill} }
\vspace{2mm} \hbox to 6.28in { {\it Student: #3 \hfill} }
\vspace{2mm} } }
\end{center}
\markboth{#1}{#1} \vspace*{4mm} }


\begin{document}

\homework{Bayesian Methods}{Changshui Zhang
  \hspace{5mm} {\tt zcs@mail.tsinghua.edu.cn}}{Xuefei Ning \hspace{5mm} {\tt foxdoraame@gmail.com
 } }{8}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 2.  Problem
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{MLE and MAP}

Maximum Likelihood Estimation (MLE) and Maximum A Posterior (MAP) are two basic principles for
learning parametric distributions. In this problem you will derive the MLE and the MAP estimates for some
widely-used distributions.

Before stating the problems, we first give a brief review of MLE and MAP. Suppose we consider a family of
distributions (c.d.f or p.m.f.) $F:=\{f(x|\theta):\theta\in\Theta\}$, where x denotes the random vector, $\theta$
denotes a vector of parameters, and $\Theta$ denotes the set of all possible values of $\theta$. Given a set
$\{x_1,x_2,...,x_n\}$ of sample points independently drawn from some $f^*\in F$, or equivalently some $f(x|\theta^*)$
such that $\theta^*\in \Theta$, we want to obtain an estimate of the value of $\theta^*$. Recall that in the case
of an independently and identically distributed(i.i.d.) sample the log-likelihood function is in the following
form
\begin{equation}
l(\theta)=\sum_{i=1}^n \log f(x_i|\theta),
\end{equation}
which is a function of $\theta$ under some fixed sample $\{x_1,x_2,...,x_n\}$. The MLE estimate $\hat{\theta}_{mle}$ is
then defined as follows:
\begin{itemize}
  \item $\hat{\theta}_{mle}\in\Theta$,
  \item $\forall \theta\in\Theta,$ $l(\theta)\leq l(\hat{\theta}_{mle})$.
\end{itemize}

If we have access to some prior distribution $P(\theta)$ over $\Theta$, be it from past experiences or domain knowledge
or simply belief, we can think about the posterior distribution over $\Theta$:
\begin{equation}
q(\theta):=\frac{\left(\prod_{i=1}^nf(x_t|\theta)\right)p(\theta)}{z(x_1,x_2,...,x_n)},
\end{equation}
where
\begin{equation}
z(x_1,x_2,...,x_n):=\int_\Theta \left(\prod_{i=1}^nf(x_t|\theta)\right)p(\theta) d\theta.
\end{equation}
The MAP estimate $\hat{\theta}_{map}$ is then defined as follows:
\begin{itemize}
  \item $\hat{\theta}_{map}\in \Theta$,
  \item $\forall \theta \in \Theta,$ $q(\theta)\leq q(\hat{\theta}_{map})$, or equivalently,
  \begin{equation}
  l(\theta)+\log p(\theta) \leq l(\hat{\theta}_{map}) + \log p(\hat{\theta}_{map}).
  \end{equation}
\end{itemize}

1. The Poisson distribution is useful for modeling the number of events occurring within a unit time, such
as the number of packets arrived at some server per minute. The probability mass function of a Poisson
distribution is as follows:
\begin{equation}
P(k|\lambda):= \frac{\lambda^k e^{-\lambda}}{k!},
\end{equation}
where $\lambda > 0$ is the parameter of the distribution and $k\in\{0,1,2,...\}$ is the discrete random variable modeling
the number of events encountered per unit time.

1.1. Let $\{k_1,k_2,...,k_n\}$ be an i.i.d. sample drawn from a Poisson distribution with parameter $\lambda$. Derive the MLE estimate $\hat{\lambda}_{mle}$
of $\lambda$ based on this sample.

\begin{addmargin}[3em]{2em}
\textbf{The log likihood function of the samples $\{k_1, k_2, \dots, k_n\}$ is}
\[
L(\{k_i\} | \lambda) = \ln(\prod_{i=1}^n \frac{\lambda^{k_i} \exp(-\lambda)}{k_i!}) = \sum_{i=1}^n (k_i \ln(\lambda) - \lambda - \ln(k_i!))
\]
\textbf{Calculate the derivative with respect to $\lambda$, we get:}
\[
\frac{\partial L}{\partial \lambda} = \sum_{i=1}^n \frac{k_i}{\lambda} - 1 = \frac{\sum_{i=1}^N k_i}{\lambda} - n
\]
\textbf{And we can see that the second derivative of the log likelihood function is always negative as $\lambda > 0$, so this function is a concave function, and the maximum of this function is achieved at its unique extreme point, so at the extreme point we have:}
\[
\frac{\sum_{i=1}^N k_i}{\lambda} - n = 0 \leadsto \lambda_{mle} = \frac{\sum_{i=1}^N k_i}{n}
\]
\end{addmargin}

1.2. Let K be a random variable following a Poisson distribution with parameter $\lambda$. Derive its mean E[K] and variance Var[K]. Since $\hat{\lambda}_{mle}$
depends on the sample used for estimation, it is also a random variable. Derive the mean and the variance of $\hat{\lambda}_{mle}$, and compare them with E[K]
and Var[K]. What do you find?

\begin{addmargin}[3em]{2em}
  \[
  E[K] = \sum_{k=0}^{\infty} \frac{\lambda^k \exp(-\lambda)}{k!} k = \sum_{k=1}^{\infty} \lambda \frac{\lambda^{k-1} \exp(-\lambda)}{(k-1)!} = \lambda \sum_{k'=0}^{\infty} \frac{\lambda^{k'} \exp(-\lambda)}{k'!} = \lambda
  \]
  \[
  \begin{split}
    Var[K] & = E[K^2] - {E[K]}^2 = \sum_{k=0}^{\infty} \frac{\lambda^k \exp(-\lambda)}{k!} k((k-1) + 1) - \lambda^2 \\
    & = \sum_{k=0}^{\infty} \frac{\lambda^k \exp(-\lambda)}{k!} k(k-1) + E[K] - {E[K]}^2 \\
    & = \lambda^2 \sum_{k'=0}^{\infty} \frac{\lambda^{k'} \exp(-\lambda)}{k'!} + E[K] - {E[K]}^2 \\
    & = \lambda^2 + \lambda - \lambda^2 = \lambda
  \end{split}
  \]

\textbf{The expectation and variance of $\lambda_{mle}$ is:}
\[
E[\lambda_{mle}] = \frac{1}{n} \sum_{i=1}^n E(k_i) = E(k_i) = \lambda
\]

\[
Var[\lambda_{mle}] = E(\lambda_{mle}^2) - {E(\lambda_{mle})}^2 = \frac{nE(K^2) + (n^2 - n)E(K)^2}{n^2} -\lambda^2 = \frac{\lambda}{n}
\]
\textbf{From the two results above, we can see the $\lambda_{mle}$ is the unbiased estimation of $\lambda$. And as the number of samples increase, the variance of the estimation decreases, the estimation is more accurate, so this estimation is a effective estimation of $\lambda$.}
\end{addmargin}

1.3. Suppose you believe the Gamma distribution
\begin{equation}
p(\lambda) := \frac{\lambda^{\alpha-1}e^{-\lambda/\beta}}{\Gamma(\alpha)\beta^\alpha},
\end{equation}
is a good prior for $\lambda$, where $\Gamma(\cdot)$ is the Gamma function, and you also know the values of the two hyper-parameters $\alpha>1$ and
$\beta>0$. Derive the MAP estimation $\hat{\lambda}_{map}$.

\begin{addmargin}[3em]{2em}
  \textbf{The posterior probability of $\lambda$ is:}
  \[
  P(\lambda | \{k_i\}) = \frac{L(\{k_i\} | \lambda) p(\lambda)}{P(\{k_i\})}
  \]
  \textbf{In which $P(\{k_i\})$ is the evidence of the observed samples $\{k_i\}$ intergrated on all the possible value of the parameter $\lambda$, the parameter $\lambda$ is marginalized out, so it is unrelated to $\lambda$. We take the logarithm of of the above equation, get:}
  \[
  \begin{split}
  \log(P(\lambda | \{k_i\})) = \sum_{i=1}^N (k_i \ln(\lambda) - \lambda - \ln(k_i!)) + \ln(p(\lambda)) - \ln(P(\{k_i\})) \\
  = \sum_{i=1}^N (k_i \ln(\lambda) - \lambda - \ln(k_i!)) + (a-1)\ln(\lambda) - \frac{\lambda}{\beta} - \ln(\Gamma(a)) - a \ln(\beta)
  \end{split}
  \]
  \textbf{Take the derivative and make it equal to 0:}
  \[
  \begin{split}
  \frac{\partial P(\lambda | \{k_i\})}{\lambda} = \frac{\sum_{i=1}^N + a - 1}{\lambda} - (1 + \frac{1}{\beta}) = 0 \\
  \leadsto \lambda_{map} = \frac{a - 1 + \sum_{i=1}^n k_i}{n + \frac{1}{\beta}}
  \end{split}
  \]

\end{addmargin}

1.4. What happens to $\hat{\lambda}_{map}$ when the sample size n goes to zero or infinity? How do they relate to the prior distribution and $\hat{\lambda}_{mle}$?

\begin{addmargin}[3em]{2em}
  \begin{itemize}
  \item \textbf{When $n \rightarrow 0$, $\lambda_{map} \rightarrow (a-1)\beta$, this is the mode value of the prior gamma distribution (the value with the maximum prior).}
  \item \textbf{When $n \rightarrow \infty$, $\lambda_{map} \rightarrow \frac{\sum_{i=1}^n k_i}{n}$, this is the maximum-likelihood estimation of $\lambda$: $\lambda_{mle}$.}
  \end{itemize}
  \quad \textbf{We can see from the two limiting case: when there are few samples, the prior distribution contributes much to the MAP estimation; while as the number of samples increase, the effect of the prior distribution fades out, and the actual samples contributes more to the MAP estimation, which will drive the MAP estimation to $\lambda_{mle}$.}\\

  \quad \textbf{Also, we can notice that the gamma distribution $\mbox{Gamma}(a, \beta)$ is the \textit{conjugate prior} of the poison likelihood function: the posterior distribution of $\lambda$ is of the same distribution family as the prior (both Gamma distribution). So the sample-adding process can be regard as an progressively parameter adjusting process of the gamma distribution (adding a new sample just ajust $a$ and $\beta$ a little). This perspective will make the MAP estimation easy to calculate progressively(at every step, the last posterior distribution can be regard as a new prior), and unrelated to the exact number of samples $n$ that have been observed.}

\end{addmargin}

2. The density function of a $p-$dimensional Gaussian distribution is as follows,
\begin{equation}
~N(x|\mu,\Lambda^{-1}):=\frac{ \exp(-\frac 1 2) (x-\mu)^T\Lambda(x-\mu)} { (2\pi)^{p/2}\sqrt{|\Lambda^{-1}|} },
\end{equation}
where $\Lambda$ is the inverse of the covariance matrix,
or the so-called precision matrix. Let $\{x_1,x_2,...,x_n\}$
be an i.i.d. sample from a $p-$dimensional Gaussian distribution.

2.1. Suppose that $n\gg p$. Derive the MLE estimates
$\hat{\mu}_{mle}$ and $\hat{\Lambda}_{mle}$.

\begin{addmargin}[3em]{2em}
  \textbf{The log likelihood function is:}
    \[
  \begin{split}
    L(X) = P(\{x_1, \dots, x_n\} | \mu, \Lambda^{-1}) & = \prod_{i=1}^{n} P(x_i | \mu, \Lambda) \\
    & = \prod_{i=1}^{n} \frac{1}{\sqrt{{(2\pi)}^p|\Lambda^{-1}|}} \exp(-\frac{1}{2} {(x_i - \mu)}^T \Lambda (x_i - \mu))\\
    & = \frac{1}{{(2\pi)}^{\frac{pn}{2}}} {\Lambda}^{\frac{n}{2}} \exp(-\frac{1}{2} \sum_{i=1}^n {(x_i - \mu)}^T \Lambda (x_i - \mu)) \\
    \mbox{logL}(X) = \log(L(X)) & = \frac{n}{2} \log(|\Lambda|) - \frac{pn}{2} \log(2\pi) - \frac{1}{2} \sum_{i=1}^n {(x_i - \mu)}^T \Lambda (x_i - \mu)
  \end{split}
  \]
  \textbf{To do the MLE estimation of $\mu$, we take the partial derivative:}
  \[
  \frac{\partial \mbox{logL(X)}}{\partial \mu} = \sum_{i=1}^n \Lambda (x_i - \mu) = \Lambda \sum_{i=1}^n (x_i - \mu) = 0  \]
  \textbf{As $\Lambda^{-1}$ is non-singular, its null space is empty set. So, we must have:}
  \[
  \sum_{i=1}^n (x_i - \mu) = 0 \leadsto \mu_{mle} = \frac{1}{n} \sum_{i=1}^n x_i
  \]
  \textbf{To do the MLE estimation of $\Lambda$, we take the partial derivative:}
  \[
  \frac{\partial \mbox{logL(X)}}{\partial \Lambda} = \frac{n}{2} \Lambda^{-T} - \frac{1}{2} \sum_{i=1}^n (x_i - \mu)(x_i - \mu)^T = 0
  \]
  \textbf{As $\Lambda$ is non-singular and symmetric, we get:}
  \[
  \Lambda_{mle} = {(\frac{1}{n} \sum_{i=1}^n (x_i - \mu_{mle}){(x_i - \mu_{mle})}^T)}^{-1}
  \]
\end{addmargin}

2.2. Suppose you believe the Gaussian-Wishart prior defined as
\begin{equation}
gw(\mu,\Lambda):= ~N(\mu|\mu_0,(s\Lambda)^{-1})W(\Lambda|V,v)
\end{equation}
is a good prior for $\mu$ and $\Lambda$, where
\begin{equation}
W(\Lambda|V,v) := \frac{|\Lambda|^{(v-p-1)/2}}{Z(V,v)} \exp \left(-\frac{tr(V^{-1}\Lambda)}{2} \right)
\end{equation}
with $tr(\cdot)$ being the trace of a square matrix and $Z(V,v)$ the normalization term. You also know the values of the hyper-parameters $\mu_0\in\mathbb{R}^p,s>0,v>p+1$, and $V\in\mathbb{R}^{p\times p}$ being positive definite. Derive the MAP estimates $\hat{\mu}_{map}$ and $\hat{\Lambda}_{map}$.

\begin{addmargin}[3em]{2em}
  \textbf{The log posterior function is:}
  \[
  \begin{split}
    \log(L(X)) & + \log(gw(\mu, \Lambda)) - \log(P(X))\\
    & = C + \frac{v-p+n}{2}\ln(|\Lambda|) - \frac{1}{2} \sum_{i=1}{n} (x_i - \mu)^T \Lambda (x_i - \mu) - \frac{1}{2}s(\mu - \mu_0)^T \Lambda (\mu - \mu_0) - \frac{\mbox{tr}(V^{-1}\Lambda)}{2}
  \end{split}
  \]
  \textbf{$C$ is a constant that do not influence the deriviative. The MAP estimation of $\mu$ satisify:}
  \[
  \frac{\partial \mbox{posterior}}{\partial \mu} = \sum_{i=1}^n \Lambda (x_i - \mu) - s \Lambda (\mu - \mu_0) = \Lambda (\sum_{i=1}^n x_i + s\mu_0 - (n+s)\mu) = 0
  \]
  \textbf{As $\Lambda$ is non-singular, we have $\mu_{map} = \frac{\sum_{i=1}^n x_i + s\mu_0}{n + s}$.}
  \textbf{The MAP estimation of $\Lambda$ statisfy:}
  \[
  \frac{\partial \mbox{posterior}}{\partial \Lambda} = \frac{v - p + n}{2}\Lambda^{-T} - \frac{1}{2} \sum_{i=1}^n (x_i - \mu)(x_i - \mu)^T - \frac{1}{2} s (\mu - \mu_0)(\mu - \mu_0)^T - \frac{V^{-T}}{2} = 0
  \]
  \textbf{Substitute in $\mu_{map}$, we get:}
  \[
  \Lambda_{map} = {(\frac{\sum_{i=1}^n (x_i - \mu_{map}){(x_i - \mu_{map})}^T + s(\mu_{map} - \mu_0) (\mu_{map} - \mu_0)^T + V^{-T}}{n + v - p})}^{-1}
  \]

\end{addmargin}

2.3. Again, what happens to $\hat{\mu}_{map}$ and $\hat{\Lambda}_{map}$ when n goes to zero or infinity?
How do they relate to the prior distribution and the MLE
estimates?

\begin{addmargin}[3em]{2em}
  \begin{itemize}
  \item \textbf{When $n \rightarrow 0$, $\mu_{map} \rightarrow \mu_0$; $\Lambda_{map} \rightarrow \frac{V}{v-p}$.}
  \item \textbf{When $n \rightarrow \infty$, $n >> s, \mu_{map} \rightarrow \frac{\sum_{i=1}^n x_i}{n} = \mu_{mle}$; substitute $\mu_{map}$ into $\Lambda_{map}$, also use $n>>s$, we have$\Lambda_{map} \rightarrow \Lambda_{mle}$.}
  \end{itemize}
  \textbf{Very intuitively, when there are few samples, the piror distribution of $\mu$ and $\Lambda$ contributes much to the posterior distribution, so when $n = 0$, the MAP estimation is just the mode value of the prior distribution of the parameters. However, as $n$ increases, the samples $\{x_i\}$ contribute more and more to the posterior distribution, and when $n \rightarrow \infty$, the influence of the prior distribution fades out, so the MAP estimation when $n \rightarrow \infty$ will approach the MLE estimation where parameter prior is not used.}\\

  \textbf{Also, the Gaussian-Wishart prior is the conjugate prior distribution of the Gaussian likelihood function, so we can see the sample-adding process as a progressively ajustment process of the hyper-parameters $V, v, s, \mu_0$ to get the posterior distribution. This property leads to quicker and simpler posterior calculation and hence MAP estimation.}
\end{addmargin}

3. It is known that MLEs do not always exist. Even if they do, they may not be unique.

3.1. Give an example where MLEs do not exist.

% http://stats.stackexchange.com/questions/133347/ml-estimate-of-exponential-distribution-with-censored-data
% http://www.itl.nist.gov/div898/handbook/apr/section4/apr412.htm
\begin{addmargin}[3em]{2em}
  \textbf{Suppose we have a failure model of a certian type of device, we use exponential distribution to model the failure process of this type of device} ($F_{\mbox{failure}}(t) = P(\mbox{failure\_time} <= t) = 1 - \exp(-\lambda t)$), \textbf{in which the failure rate of this type of device remains unchanged during the lifetime of the device:}
  \[
  \lambda(t) = \frac{f(t)}{1 - F(t)} = \lambda
  \]
  \textbf{Suppose we observe the behavior of $N$ such devices for a period: $T$. We assume in this time period, there are $n$ device failure observed, we assume the $n$ devices failed at $t_1, t_2, \dots, t_n$. We can write the likelihood function of these observation as:}
  \[
  \mbox{logL} = \log(\prod_{i=1}^n \lambda \exp(-\lambda t_i) {(1-F(T))}^{N-n}) = n \ln(\lambda) - \lambda \sum_{i=1}^n t_i - (N-n) \lambda T
  \]
  \textbf{When $n = 0$, the log likelihood function is:}
  \[
  \mbox{logL}(\lambda) = - N \lambda T
  \]
  \textbf{The maximum is achieved at $\lambda = 0$, however, $\lambda = 0$ does not correspond to a legal exponential distribution, because every exact point have a probability of 0. So, in this situation, the MLE estimation do not exist.}
\end{addmargin}

3.2. Give an example where MLEs exist but are not unique. Please specify the family of distributions being considered, and the kind of samples from which multiple MLEs can be found.

\begin{addmargin}[3em]{2em}
  \textbf{Consider a mixture of two fixed-mean fixed-variance gaussian distribution with the same $\Sigma$, the parameter $\theta, 0 \leq \theta \leq 1$ is the combination factor of the two gaussian probabilistic function:}
  \[
  p(x | \theta) = \theta \frac{1}{{(2\pi)}^{n/2}|\Sigma^{-1}|} \exp(-\frac{(x - \mu_1)^T \Sigma^{-1} (x - \mu_1)}{2}) + (1 - \theta) \frac{1}{{(2\pi)}^{n/2}|\Sigma^{-1}|} \exp(-\frac{{(x - \mu_2)}^T \Sigma^{-1} (x - \mu_2)}{2})
  \]

  \textbf{When the observed sample $x_s$ is at the middle point of $\mu_1$ and $\mu_2$, that is $x_s = \frac{\mu_1 + \mu_2}{2}$, the MLE of the parameter $\theta$ is not unique, because $\theta$ can be every value in $[0, 1]$, and the likelihood of this special sample won't change.}

\end{addmargin}

3.3. By finding the two examples as described above, hopefully you have gained some intuition on the properties of the loglikelihood that are crucial to the existence and uniqueness of MLE. What are those properties?

\begin{addmargin}[3em]{2em}
  \textbf{To guarantee the MLE of parameter always exists and its uniqueness, the log likelihood function should have the only maximum point no matter what samples are observed.}
\end{addmargin}

4. Consider a training data of $N$ i.i.d. (independently and identically distribute) observations, $\bm X=\{x_1, x_2, ..., x_N\}$ with corresponding $N$ target values $\bm T=\{t_1, t_2, ..., t_N\}$.

We want to fit these observations into some model
\begin{equation}\label{eq1}
t = y(x, \bm w) + \epsilon
\end{equation}

where $\bm w$ is the model parameters and $\epsilon$ is some error term.

4.1 To find $\bm w$, we can minimize the sum of square error
\begin{equation}\label{eq2}
E(\bm w) = \frac{1}{2}\sum_{n=1}^N\{y(x_n, \bm w)-t_n\}^2
\end{equation}

Now suppose we believe that the distribution of error term $\epsilon$ is gaussian
\begin{equation}\label{eq3}
p(\epsilon|\beta) = \mathcal N(\epsilon|0, \beta^{-1})
\end{equation}

where $\beta = \frac{1}{\sigma^2}$ is the inverse of variance. Using the property of gaussian distribution, we have
\begin{equation}\label{eq4}
p(t|x, \bm w, \beta) = \mathcal N(t|y(x, \bm w), \beta^{-1})
\end{equation}

Under this assumption, the likelihood function is given by

\begin{equation}\label{eq5}
p(\bm T|\bm X, \bm w, \beta) = \prod_{n=1}^N \mathcal N(t_n|y(x_n, \bm w), \beta^{-1})
\end{equation}

Show that the problem of finding the maximum likelihood (ML) solution for $\bm w$ is equivalent to the problem of minimizing the sum of square error (\ref{eq2}).

\begin{addmargin}[3em]{2em}
  \textbf{The log likelihood of the observation $T=\{t_i\}$ is:}
  \[
  \log(p(T | X, \omega, \beta)) = -N \ln(\frac{2 \pi}{\sqrt{\beta}}) - \frac{\beta}{2} \sum_{i=1}^N {(t - y(x, \omega))}^2
  \]
  \textbf{In maximum likelihood estimation, we maximize the log likelihood function, which is to minimize $\sum_{i=1}^N {(t - y(x, \omega))}^2$ according to the equation above. So this is equivalent to minimizing the sum of square error.}
\end{addmargin}

4.2 In order to avoid overfitting, we often add a weight decay term to (\ref{eq2})

\begin{equation}\label{eq6}
  E(\bm w) = \frac{1}{2}\sum_{n=1}^N\{y(x_n, \bm w)-t_n\}^2 + \frac{1}{2}||\bm w||^2
\end{equation}

On the other hand, we believe that $\bm w$ has a prior distribution of
\begin{equation}\label{eq7}
  p(\bm w|\alpha) = \mathcal N(\bm w|\bm 0, \alpha^{-1}\bm I)
\end{equation}

Using Bayes’ theorem, the posterior distribution for $\bm w$ is proportional to the product of the prior distribution and the likelihood function
\begin{equation}\label{eq8}
  p(\bm w|\bm X, \bm T, \alpha, \beta) \propto p(\bm T|\bm X, \bm w, \beta)p(\bm w|\alpha)
\end{equation}

Show that the problem of finding the maximum of the posterior (MAP) solution for $\bm w$ is equivalent to the problem of minimizing (\ref{eq6}).


\begin{addmargin}[3em]{2em}
  \[
  \log(p(\omega)) \propto \log(p(T | X, \omega, \beta)) + \log(p(\omega | \alpha)) = C - \frac{\beta}{2} \sum_{i=1}^N {(t - y(x, \omega))}^2 - \alpha \frac{\omega^T \omega}{2}
  \]
  \textbf{Doing the MAP estimation of the posterior distribution of $\omega$, is equivalent to minimize $\frac{\beta}{2} \sum_{i=1}^N {(t - y(x, \omega))}^2 + \alpha \frac{\|\omega\|}{2}$, this is the form of the square error loss with a weight decay regularizing term (when $\alpha = \beta$).}
\end{addmargin}

\section*{Naive Bayes}

1. Considers the learning function $X \rightarrow Y$, where class label $Y\in\{T,F\}$, $X=\langle X_1,X_2,...,X_n \rangle$ where $X_1$ is a boolean variable and $\{X_2,...,X_n\}$ are continuous variables. Assume that for each continuous $X_i$, $P(X_i|Y=y)$ follows a Gaussian distribution. List and give the total number of the parameters that you would need to estimate in order to classify a future example using a Naive Bayes classifier.
Give the formula for computing $P(Y|X)$ in terms of these
parameters and feature variables $X_i$.

\begin{addmargin}[3em]{2em}
  \textbf{When $Y = T$, we need a parameter $P(X_1 = T | Y = T)$ for $X_1$, and two parameter $\mu_i, \sigma_i$ for every other continous variable $X_i$. And another set of parameters are needed when $Y = F$. So we need $2(1 + 2(n-1)) = 2(2n-1)$ parameters to build the Naive Bayes classifier.}\\

  \[
  P(Y=y|X) = \frac{P(X|Y)P(Y)}{P(X)} = \frac{P(X_1|Y=y) \prod_{i=2}^n P(X_i|Y) P(Y)}{P(X_1|Y=T) \prod_{i=2}^n P(X_i|Y=T) + P(X_1|Y=F) \prod_{i=2}^n P(X_i|Y=F) }
  \]
  \textbf{In the above equation, $P(Y)$ prior is estimated using the occuring times of $Y=T$ and $Y=F$ in the training data.}
\end{addmargin}

2. Consider a simple learning problem of determining whether Alice and Bob from CA will go to hiking or not
$Y:Hike\in\{T,F\}$ given the weather conditions $X_1:Sunny\in\{T,F\}$ and $X_2:Windy\in\{T,F\}$ by a
Naive Bayes classifier. Using training data, we estimated the parameters $P(Hike) = 0.5$, $P(Sunny|Hike) = 0.9$, $P(Windy|\neg Hike) = 0.8$, $P(Windy|Hike) = 0.3$ and $P(Sunny|\neg Hike) = 0.4$. Assume that the true distribution of $X_1$, $X_2$, and $Y$ satisfies the Naive Bayes
assumption of conditional independence with the above parameters.

2.1. Assume Sunny and Windy are truly independent given Hike. Write down the Naive Bayes
decision rule for this problem using both attributes Sunny and Windy.
\begin{addmargin}[3em]{2em}
  \textbf{We use $S$ for the abbreviation of the random variable $Sunny$, $W$ for $Windy$ and $H$ for $Hike$. The posterior distribution of $H$ is:}
  \[
  P(H | S, W) = \frac{P(S, W | H)P(H)}{P(S, W, H)} = \frac{P(S | H)P(W | H)P(H)}{P(S|H=T)P(W | H = T)P(H=T) + P(S|H=F)P(W | H = F)P(H=F)}
  \]
  \textbf{The decision rule is:}
  \[
  \begin{cases}
    \hat{H}=T: P(H=T|S,W) > P(H=F|S,W) \leadsto P(S|H=T)P(W|H=T) > P(S|H=F)P(W|H=F)\\
    \hat{H}=F: \mbox{otherwise}
  \end{cases}
  \]
  \textbf{The four cases is listed below:}
  \begin{itemize}
  \item $S=T, W=T$, $0.27 = P(S=T|H=T)P(W=T|H=T) < P(S=T|H=F)P(W=T|H=F) = 0.32$, so the decision is $\hat{H} = F$.
  \item $S=T, W=F$, $0.63 = P(S=T|H=T)P(W=F|H=T) > P(S=T|H=F)P(W=F|H=F) = 0.08$, so the decision is $\hat{H} = T$.
  \item $S=F, W=T$, $0.03 = P(S=F|H=T)P(W=T|H=T) < P(S=F|H=F)P(W=T|H=F) = 0.48$, so the decision is $\hat{H} = F$.
  \item $S=F, W=F$, $0.07 = P(S=F|H=T)P(W=F|H=T) > P(S=F|H=F)P(W=F|H=F) = 0.12$, so the decision is $\hat{H} = F$.
  \end{itemize}
\end{addmargin}

2.2. Given the decision rule above, what is the expected error rate of the Naive Bayes classifier? (The
expected error rate is the probability that each class generates an observation where the decision
rule is incorrect.)
\begin{addmargin}[3em]{2em}
  \[
  \begin{split}
    P(\mbox{error}) = & \sum_{s, w \mbox{ s.t. } \hat{H}=T}P(H=F|S=s,W=w)P(S=s, W=w)\\
    & + \sum_{s, w \mbox{ s.t. } \hat{H} = F}P(H=T|S=s, W=w)P(S=s, W=w) \\
    = & \sum_{s, w \mbox{ s.t. } \hat{H}=T}P(S=s,W=w|H=F)P(H=F)\\
    & + \sum_{s, w \mbox{ s.t. } \hat{H}=F}P(S=s,W=w|H=T)P(H=T) \\
    = & 0.08 * 0.5 + (0.27 + 0.03 + 0.07) * 0.5 = 0.225
  \end{split}
  \]
\end{addmargin}

2.3. What is the joint probability that Alice and Bob go to hiking and the weather is sunny and
windy, that is $P(Sunny,Windy,Hike)$?
\begin{addmargin}[3em]{2em}
  \textbf{ The joint probability is:}
\[
P(S=T,W=T,H=T) = P(H=T)p(S=T|H=T)P(W=T|H=T) = 0.135
\]
\end{addmargin}

2.4. Next, suppose that we gather more information about weather conditions and introduce a new feature
denoting whether the weather is $X_3$: Rainy or not. Assume that each day the weather in CA can
be either Rainy or Sunny. That is, it can not be both Sunny and Rainy (similarly, it can not be
$\neg Sunny$ and $\neg Rainy$). In the above new case, are any of the Naive Bayes assumptions violated? Why (not)? What is
the joint probability that Alice and Bob go to hiking and the weather is sunny, windy and not
rainy, that is $P(Sunny,Windy,\neg Rainy,Hike)$?

\begin{addmargin}[3em]{2em}
  \textbf{The naive bayes assumption is violated, because the $\mbox{Sunny}$ random variable is dependent to $\mbox{Rainy}$ variable even if the decision is given. The joint probability remains unchange here:}
  \[
  P(S=T,W=T,H=T) = P(H=T)p(S=T|H=T)P(W=T|H=T) = 0.135
  \]
\end{addmargin}[3em]{2em}

2.5. What is the expected error rate when the Naive Bayes classifier uses all three attributes? Does
the performance of Naive Bayes improve by observing the new attribute Rainy? Explain why.

\begin{addmargin}[3em]{2em}
  \textbf{The error rate remains unchanged as $0.225$, and the performance of Naive Bayes does not improve by observing the new attribute Rainy, because $\mbox{Rainy} = \neg \mbox{Sunny}$, and it brough no additional information.}
\end{addmargin}


\section*{Programming}
In this problem you will implement Naive Bayes and Logistic Regression, then compare their performance on a
document classification task. The data for this task is taken from the 20 Newsgroups data set, and is available
from the attached zip file. The included README.txt describes the data set and file format.

Our Naive Bayes model will use the bag-of-words assumption. This model assumes that each word in a
document is drawn independently from a multinomial distribution over possible words. (A multinomial
distribution is a generalization of a Bernoulli distribution to multiple values.) Although this model ignores
the ordering of words in a document, it works surprisingly well for a number of tasks. We number the words
in our vocabulary from 1 to $m$, where $m$ is the total number of distinct words in all of the documents.
Documents from class $y$ are drawn from a class-specific multinomial distribution parameterized by $\theta_y$. $\theta_y$ is
a vector, where $\theta_{y,i}$ is the probability of drawing word $i$ and $\sum_{i=1}^m \theta_{y,i}=1$.
Therefore, the class-conditional probability of drawing document x from our Naive Bayes model is
$P(X = x|Y = y) = \prod_{i=1}^m (\theta_{y,i})^{count_i(x)}$, where $count_i(x)$ is the number of times word $i$ appears in $x$.

1. Provide high-level descriptions of the Naive Bayes and Logistic Regression algorithms. Be
sure to describe how to estimate the model parameters and how to classify a new example.

2. Imagine that a certain word is never observed in the training data, but occurs in a test
instance. What will happen when our Naive Bayes classifier predicts the probability of the this test
instance? Explain why this situation is undesirable. How to avoid this problem? Will logistic regression have a similar problem?
Why or why not?

3. Implement Logistic Regression and Naive Bayes. Use add-one smoothing when estimating
the parameters of your Naive Bayes classifier. For logistic regression, we found that a step size around
0.0001 worked well. Train both models on the provided training data and predict the labels of the test
data. Report the training and test error of both models. Submit your code along with your homework.

4. Which model performs better on this task? Why do you think this is the case?


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Reference
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{thebibliography}{1}

%\bibitem{BoydVandenberghe2004}
%S. Boyd and L. Vandenberghe, \emph{Convex Optimization}, Cambridge
%University Press, 2004.

%\end{thebibliography}
\end{document}
